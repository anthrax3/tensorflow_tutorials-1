{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Temelleri ve Doğrusal Regresyon\n",
    "Bu Jupyter Notebook tensorflow'un temel prensiplerini ve basit bir doğrusal regresyon modelini içermektedir. \n",
    "Bu notebook doğrusal regresyonu açıklama amacı gütmez. Bu işlemlerin arkasında yatan matematiği ve mantığı öğrenmek için internetteki birçok doğrusal regresyon kaynağına bakabilirsiniz.\n",
    "\n",
    "Bu notebook'ta, doğrusal regresyon parametreleri 'Gradient Descent' yöntemiyle öğrenilecektir. En çok kullanılan doğrusal regresyon öğrenim yöntemi sıradan en küçük kareler (ordinary least squares) yöntemi olsa da, ilerideki optimizasyon bölümlerine önayak olması açısından gradient descent kullanmayı uygun gördüm. Siz isterseniz OLS yöntemini kendiniz implement edebilirsiniz.\n",
    "\n",
    "Bu notebook'taki aşamalar şu şekildedir:\n",
    "\n",
    "- Libraryleri yükle\n",
    "- Veriyi yükle\n",
    "- *Tensorflow işlemlerini ve işlemlerin oluşturduğu graph'ı tanımla*\n",
    "- *Tensorflow session'ını çalıştır.*\n",
    "\n",
    "Aynı aşamaları sonrasında Mini-batch Gradient Descent ile tekrarlayıp, graph'ımızı [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) ile görselleştiriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elvan/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraryleri yükle\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import tensorflow as tf # RuntimeWarning çıkarsa sorun değil\n",
    "import pandas as pd\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0        1     2     3    4      5      6     7       8    9      10  \\\n",
       "0  1.0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  1.0  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  1.0  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  1.0  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  1.0  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "     11      12    13  \n",
       "0  15.3  396.90  4.98  \n",
       "1  17.8  396.90  9.14  \n",
       "2  17.8  392.83  4.03  \n",
       "3  18.7  394.63  2.94  \n",
       "4  18.7  396.90  5.33  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veriyi oku\n",
    "boston = load_boston() # boston.data ve boston.target olmak üzere iki array içerir\n",
    "m, n = boston.data.shape # verinin satır sütun sayıları\n",
    "print(m, n)\n",
    "boston_plus_bias = np.c_[np.ones((m, 1)), boston.data]\n",
    "pd.DataFrame(boston_plus_bias).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Görülüğü üzere, veriyi ilk yüklediğimizde 506 satır, 13 sütunluk bir verisetine sahiptik. \n",
    "\n",
    "Ancak doğrusal regresyon ve yapay sinir ağları modelleri için bir 'bias' gerekmektedir. 'Bias' dediğimiz kısım, sabit bir değer olup, doğrusal regresyon için 'intercept' değerine denk gelmektedir. Bu sabit değeri öğrenebilmek için verimize 1 sayısından oluşan bir sütun ekliyoruz. Bu sütuna denk gelecek ağırlık bizim 'bias' değerimiz olacaktır.\n",
    "\n",
    "Bu noktadan sonra, verimize eriştiğimize göre Tensorflow değişkenlerini tanımaya ve graph'ımızı tanımlamaya geçebiliriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow'un Çalışma Mantığı\n",
    "\n",
    "Tensorflow yapısı gereği önce değişkenleri ve değişkenlere yapılacak işlemleri tanımlamanızı gerektirir. Sonrasında ise bu işlemleri koşturarak bir sonuç üretir. Bu aşamaların sebebi, tensorflow'un öncelikle bir koşturma grafiği oluşturmasıyla alakalıdır. Bu grafik sayesinde Tensorflow, grafiğin parçalarını paralel çalıştırarak hız ve performans kazanır. \n",
    "\n",
    "Bu yöntemin dezavantajı ise interaktivite kaybıdır. Normal Python kodları gibi interaktif çalışmak problemli olabilir. Bunun için üretilmiş daha üst seviye kütüphaneler mevcuttur. En çok bilineni [Keras](https://keras.io/) olan bu kütüphanelerin bir benzerini de Google [Tensorflow Eager](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager) adıyla çıkarmıştır ancak benim henüz bu kütüphaneyi deneme fırsatım olmadı. Daha basit bir çalışma süreci isterseniz Keras kullanabilirsiniz ancak bu notebooklar ana Tensorflow'a odaklanmaktadır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Değişkenleri tanımlar\n",
    "X = tf.constant(boston_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(boston.target.reshape(-1,1), dtype=tf.float32, name = \"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1, 1), name = \"theta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanımladığımız değişkenlerden X, y sabit; theta ise değişkendir (Variable). Bunun sebebi, verimiz olan X ve y'nin değişmez olması; öğrenmek istediğimiz değişken olan theta'nın ise kodun çalışma süresi boyunca değişecek olmasıdır.\n",
    "\n",
    "Şimdi öğrenme parametrelerini tanımlayalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 2000 #2000 iterasyon. \n",
    "# Kendi bilgisayarınıza göre düşürüp arttırabilirsiniz.\n",
    "\n",
    "learning_rate = 0.000001 # Gradient descent adım boyutu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu parametreler çoğunlukla deneme yanılma yöntemi ve öğrenme performansına bakılarak belirlenir. Eğer Gradient Descent yöntemini bilmiyorsanız öğrenmek için Fast.ai [wiki](http://wiki.fast.ai/index.php/Gradient_Descent)'sine bakmanızı tavsiye ederim.\n",
    "\n",
    "Bu noktadan sonra yapmamız gereken şey, Tensorflow'un yapacağı işlemleri tanımlamak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu noktada, Tensorflow'un yapacağı işlemleri tanımladık ancak henüz hiçbir işlem yapılmadı. \n",
    "\n",
    "İlk satırda, tensorflow'a tahminleri nasıl hesaplayacağını söylüyoruz.\n",
    "\n",
    "İkinci satırda, hatayı nasıl hesaplayacağını söylüyoruz.\n",
    "\n",
    "Üçüncü satırda tüm kare hataların ortalamasını nasıl alacağını söylüyoruz.\n",
    "\n",
    "Dördüncü satırda, Gradient Descent optimizasyonunu kullanmasını ve bu optimizasyonun adım boyutunu belirtiyoruz.\n",
    "\n",
    "Beşinci satırda optimizasyonun, ortalama kare hatayı minimize ederek yapılacağını söylüyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İşlemleri gerçekleştirmek için öncelikle bir Tensorflow session'ı oluşturmamız gerekiyor. Bu session graph'ı oluşturarak işlemleri koşturacak ve bu sayede sonuçları elde edebileceğiz. \n",
    "\n",
    "Henüz ne değişkenler ne de işlemler başlatılmadı. İlk yapacağımız şey değişkenleri başlatıp, sonrasında işlemleri koşturmak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  227182.0\n",
      "Epoch 100 MSE =  583.323\n",
      "Epoch 200 MSE =  518.257\n",
      "Epoch 300 MSE =  464.711\n",
      "Epoch 400 MSE =  420.195\n",
      "Epoch 500 MSE =  382.842\n",
      "Epoch 600 MSE =  351.227\n",
      "Epoch 700 MSE =  324.259\n",
      "Epoch 800 MSE =  301.089\n",
      "Epoch 900 MSE =  281.055\n",
      "Epoch 1000 MSE =  263.632\n",
      "Epoch 1100 MSE =  248.4\n",
      "Epoch 1200 MSE =  235.021\n",
      "Epoch 1300 MSE =  223.22\n",
      "Epoch 1400 MSE =  212.769\n",
      "Epoch 1500 MSE =  203.48\n",
      "Epoch 1600 MSE =  195.195\n",
      "Epoch 1700 MSE =  187.782\n",
      "Epoch 1800 MSE =  181.127\n",
      "Epoch 1900 MSE =  175.135\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # Değişkenleri başlat\n",
    "    \n",
    "    # Optimizasyonu epoch sayısı kadar çalıştır\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0: # Her 100 epoch'te bir performans çıktısı\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent söz konusu olduğu zaman dikkat edilmesi gereken en büyük nokta öğrenme adımının boyutudur. Eğer öğrenme adımınız çok büyükse 'Exploding weights' (büyüyen ağırlıklar) problemiyle karşılaşırsınız. Bu durumda, öğrenme sürecinde hatanız azalmak yerine sonsuza gidecektir. İsterseniz bu notebook'te learning_rate'i değiştirerek deneyebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "\n",
    "Bir önceki optimizasyon yönteminin ufak bir sorunu var: Parametreleri öğrenebilmek için bütün veriyi kullanıyor. Bu kadar ufak bir veri için bu bir sorun değil. Ancak ileride, RAM'de tutamayacak kadar büyük bir veri olduğu durumda, veriyi parça parça okuyarak işlemleri gerçekleştirmek gerekiyor. Bu yüzden kodu biraz değiştirmemiz gerek.\n",
    "\n",
    "Tensorflow'a her bir batch iterasyonunda yeni okuduğumuz verileri iletebilmek için 'placeholder' adı verilen değişkenler oluşturmamız gerekiyor. Aslında her tensorflow değişkenine yeni veri iletebilirsiniz ancak en doğru yolu placeholderları kullanmak. Placeholderlar kodu veri olmadan koşturmaya çalıştığınızda hata verir, bu sayede veri beslemeyi unutmazsınız.\n",
    "\n",
    "Hazır kodu değiştirmişken graph'ımızı görselleştirmek için bir loglama prosedürü de ekleyelim. Sonrasında Tensorboard ile performans değişimine bakabiliriz.\n",
    "\n",
    "Graph'ın kalabalıklaşmaması için graph'ı öncelikle resetliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape = (None, n+1), name = \"X\")\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = \"y\")\n",
    "\n",
    "# Parça boyutu ve sayısı\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size)) # Python'un int-float kurallarına dikkat\n",
    "\n",
    "# Parçaları çağırmak için fonksiyonu tanımla\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = boston_plus_bias[indices] # Buradaki veri değişebilir\n",
    "    y_batch = boston.target.reshape(-1, 1)[indices] \n",
    "    y_batch = y_batch.reshape(-1,1)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loglama için gerekli kütüphaneleri yükleyip, değişkenleri oluşturuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d&H%M%S\")\n",
    "root_logdir = \"tf_logs_lr\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "n_epochs = 2000 \n",
    "learning_rate = 0.000001 # Gradient descent adım boyutu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 14)\n",
      "(100, 14)\n",
      "(100, 14)\n",
      "(100, 14)\n",
      "(100, 14)\n",
      "(100, 14)\n"
     ]
    }
   ],
   "source": [
    "for batch_index in range(n_batches):\n",
    "    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "    print(X_batch.shape)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  559260.0\n",
      "Epoch 100 MSE =  58005.2\n",
      "Epoch 200 MSE =  9470.13\n",
      "Epoch 300 MSE =  1473.75\n",
      "Epoch 400 MSE =  680.29\n",
      "Epoch 500 MSE =  737.706\n",
      "Epoch 600 MSE =  771.147\n",
      "Epoch 700 MSE =  474.938\n",
      "Epoch 800 MSE =  632.57\n",
      "Epoch 900 MSE =  688.002\n",
      "Epoch 1000 MSE =  672.85\n",
      "Epoch 1100 MSE =  622.763\n",
      "Epoch 1200 MSE =  526.597\n",
      "Epoch 1300 MSE =  519.646\n",
      "Epoch 1400 MSE =  617.421\n",
      "Epoch 1500 MSE =  526.324\n",
      "Epoch 1600 MSE =  543.233\n",
      "Epoch 1700 MSE =  628.899\n",
      "Epoch 1800 MSE =  583.758\n",
      "Epoch 1900 MSE =  403.859\n"
     ]
    }
   ],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1, 1), name = \"theta\")\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Loglama bilgilerinin değişkenlerini oluştur\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # Değişkenleri başlat\n",
    "    \n",
    "    # Optimizasyonu epoch sayısı kadar çalıştır\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if(batch_index % 10 == 0):\n",
    "                summary_str = mse_summary.eval(feed_dict = {X: X_batch, y:y_batch})\n",
    "                step = epoch*n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "        if epoch % 100 == 0: # Her 100 epoch'te bir performans çıktısı\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X: X_batch, y:y_batch}))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "    file_writer.close()\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bu noktada /tf_logs_lr/ klasörüne bir 'run' ile başlayan bir klasör oluşturulmuş olması gerekiyor. Bu klasörün içindeki dosya bizim öğrenme sürecimizi ve graph'ımızı içeriyor. Bu graph'ı aşağıdaki kodla Tensorboard'a atarak inceleyebilirsiniz.\n",
    "\n",
    "Tensorboard'u aç:\n",
    "```shell\n",
    "tensorboard --logdir tf_logs_lr/\n",
    "```\n",
    "\n",
    "Sonrasında ise http://0.0.0.0:6006 adresine girerek Tensorboard'a ulaşabilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benim makinemdeki Tensorboard'ın görselleri aşağıdaki gibi.\n",
    "\n",
    "MSE: ![alt text](images/tb_scalar.png \"MSE\")\n",
    "\n",
    "Graph: ![alt text](images/tb_graph.png \"Graph\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
